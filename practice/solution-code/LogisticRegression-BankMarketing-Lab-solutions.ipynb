{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "\n",
    "# Logistic Regresion Lab\n",
    "## Exercise with bank marketing data\n",
    "\n",
    "_Authors: Sam Stack(DC)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "- Data from the UCI Machine Learning Repository: data, [data dictionary](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)\n",
    "- **Goal**: Predict whether a customer will purchase a bank product marketed over the phone\n",
    "- `bank-additional.csv` is already in our repo, so there is no need to download the data from the UCI website\n",
    "\n",
    "## Step 1: Read the data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:15.353055Z",
     "start_time": "2020-10-12T17:58:15.240051Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, log_loss\n",
    "\n",
    "bank = pd.read_csv('../../data/bank.csv')\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  Target '`y`' represented as such**\n",
    "    - No : 0\n",
    "    - Yes : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:16.443383Z",
     "start_time": "2020-10-12T17:58:16.427699Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the results of y\n",
    "bank['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Prepare at least three features\n",
    "- Include both numeric and categorical features\n",
    "- Choose features that you think might be related to the response (based on intuition or exploration)\n",
    "- Think about how to handle missing values (encoded as \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:17.048125Z",
     "start_time": "2020-10-12T17:58:17.040502Z"
    }
   },
   "outputs": [],
   "source": [
    "# I'm going to take about 6 features and build two separate models.  \n",
    "# Age, Job, Marital, education, contact, day of week.\n",
    "# A correlation matrix or heat map is probably beneficial to finding useful features.\n",
    "# This can be difficult with the amount of categorical features in the data.\n",
    "# Once converted to dummie variables that will still be a computationally expensive process\n",
    "# to compare all features.\n",
    "\n",
    "# there was no formal eda behind my selection, I just wanted to use random features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:17.275815Z",
     "start_time": "2020-10-12T17:58:17.230639Z"
    }
   },
   "outputs": [],
   "source": [
    "features = ['age','job','marital','education','contact','day_of_week','y']\n",
    "\n",
    "for feat in features:\n",
    "    if feat != 'age':\n",
    "        print(bank[feat].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qualitative data analysis**  \n",
    "So I have some unknown values in `education`, `marital` and `employment`.  We could make assumptions that the 39 unkown from `employment` are most likely in `admin` professions or that the 11 unknown in `marital` are most likely `married` (unfortunate that they are uncertain about it).\n",
    "\n",
    "Personally, im going to drop the unknowns as I do not want to encorporate any addition bias into the data itself.  \n",
    "- Going forward a more sound method of replacing unknowns is to build models to predict them using K Nearest neighbors, that way you are filling in an unknown using the most similar observations you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:17.895709Z",
     "start_time": "2020-10-12T17:58:17.877166Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating the sub dataframe with only the features we're using\n",
    "bank_a =  bank[features]\n",
    "\n",
    "# getting rid of unknowns - there are more sophisticated ways to drop these, but this works.\n",
    "\n",
    "is_ed_unk = bank_a['education'] != 'unknown'\n",
    "bank_a = bank_a[is_ed_unk]\n",
    "\n",
    "is_job_unk = bank_a['job'] != 'unknown'\n",
    "bank_a = bank_a[is_job_unk]\n",
    "\n",
    "is_married_unk = bank_a['marital'] != 'unknown'\n",
    "bank_a = bank_a[is_married_unk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready to get dummied, but i'll wait until we're about to model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Model building\n",
    "- Use cross-validation to evaluate the logistic regression model with your chosen features.  \n",
    "    You can use any (combination) of the following metrics to evaluate.\n",
    "    - [Classification/Accuracy Error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "    - [Confusion Matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "    - [ROC curves and area under a curve (AUC)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n",
    "    - [Log loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n",
    "- Try to increase the AUC by selecting different sets of features\n",
    "    - *Bonus*: Experiment with hyper parameters such are regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a Model**  \n",
    "*Model 1, using `age`, `job`, `education`, and `day_of_week`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:19.080778Z",
     "start_time": "2020-10-12T17:58:18.967134Z"
    }
   },
   "outputs": [],
   "source": [
    "# md = ModelData.  Dummies ignores numeric columns such as age and y\n",
    "bank_md1 = pd.get_dummies(bank_a[['age','job','education','day_of_week','y']], drop_first = True)\n",
    "\n",
    "\n",
    "bank\n",
    "# no hyper parameters for first model\n",
    "LogReg1 = LogisticRegression()\n",
    "\n",
    "# X and y features\n",
    "X1 = bank_md1.drop('y', axis =1)\n",
    "y1 = bank_md1['y']\n",
    "\n",
    "\n",
    "\n",
    "# using train test split to cross val\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X1,y1, random_state =42)\n",
    "\n",
    "# fit model\n",
    "LogReg1.fit(x_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the Coefficient for each feature.**\n",
    "- Be sure to make note of interesting findings.\n",
    "\n",
    "*Seems like `job_entrepreneur` carries that largest coef.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:19.479929Z",
     "start_time": "2020-10-12T17:58:19.428994Z"
    }
   },
   "outputs": [],
   "source": [
    "name = bank_md1.columns.drop('y')\n",
    "\n",
    "coef = LogReg1.coef_[0]\n",
    "\n",
    "pd.DataFrame([name,coef],index = ['Name','Coef']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the Model to predict on x_test and evaluate the model using metric(s) of Choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:19.967408Z",
     "start_time": "2020-10-12T17:58:19.951157Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict with model\n",
    "y_pred = LogReg1.predict(x_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:20.259450Z",
     "start_time": "2020-10-12T17:58:20.246199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test1,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Accuracy Score**\n",
    "\n",
    "- Wow thats a pretty good score wouldn't you say?  Almost 90!  Remember the distribution of classes though.  In our entire dataset there are 3668 \"No\" and 451 \"Yes\" and a total of 4119 observations.  If we guessed that nobody was going to convert and therefore 'No' every time, we would be correct 89% of the time (according to out data).  That being said, this accuracy is barely better than baseline and such an insignificant difference could just be from how our train test split groupped the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "Looks like we have 880 True Negatives and 99 False Negatives.  That being said it looks like all our model is doing is predicting 'no' everytime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:22.104675Z",
     "start_time": "2020-10-12T17:58:22.090865Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Confusion Matrix:\\n {confusion_matrix(y_test1,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ROC AUC**\n",
    "\n",
    "The Area Under the ROC Curve is 0.5 which is completely wothless and our model gains no more insight that random guessing.  If we go back to the Accuracy score, we can now conclude that its minuscule improvement above the baseline is caused by our train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:22.961612Z",
     "start_time": "2020-10-12T17:58:22.943914Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'ROC-AUC Score: {roc_auc_score(y_test1,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:23.878017Z",
     "start_time": "2020-10-12T17:58:23.868314Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Log Loss: {log_loss(y_test1,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Using `age`, `job`, `marital`, `education`, `contact` and `day_of_week` to predict If the bought or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:26.029579Z",
     "start_time": "2020-10-12T17:58:25.902742Z"
    }
   },
   "outputs": [],
   "source": [
    "# md = ModelData.  Dummies ignores numeric columns such as age and y\n",
    "bank_md2 = pd.get_dummies(bank_a, drop_first = True)\n",
    "\n",
    "# no hyper parameters for first model\n",
    "LogReg2 = LogisticRegression()\n",
    "\n",
    "# X and y features\n",
    "X2 = bank_md2.drop('y', axis =1)\n",
    "y2 = bank_md2['y']\n",
    "\n",
    "# using train test split to cross val\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2,y2, random_state =42)\n",
    "\n",
    "# fit model\n",
    "LogReg2.fit(x_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T17:58:26.541044Z",
     "start_time": "2020-10-12T17:58:26.526006Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred2 = LogReg2.predict(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:52.195186Z",
     "start_time": "2020-10-12T18:02:52.175643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the metrics\n",
    "print(f'Accuracy: {accuracy_score(y_test2,y_pred2)}')\n",
    "print()\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(y_test2,y_pred2)}')\n",
    "print()\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test2,y_pred2)}')\n",
    "print()\n",
    "print(f'Log Loss: {log_loss(y_test2,y_pred2)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the metrics really changed.  Looks like the features we have arn't very helpful...\n",
    "\n",
    "\n",
    "### Is your model not performing very well?\n",
    "\n",
    "Lets try one more thing before we revert to grabbing more features.  Adjusting the probability threshold.\n",
    "\n",
    "Use the `LogisticRegression.predict_proba()` attribute to get the probabilities.\n",
    "\n",
    "Recall from the lesson the first probability is the for class 0 and the second is for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:53.277930Z",
     "start_time": "2020-10-12T18:02:53.243743Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_prob = LogReg2.predict_proba(x_test2)\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:54.692311Z",
     "start_time": "2020-10-12T18:02:54.257925Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_prob_t = y_pred_prob.transpose()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(y_pred_prob_t[0])\n",
    "plt.show()\n",
    "plt.hist(y_pred_prob_t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Calculate a new threshold and use it to convert predicted probabilities to output classes**\n",
    "\n",
    "Lets try decreaseing the threshold to %20 predicted probability or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:56.296885Z",
     "start_time": "2020-10-12T18:02:56.277802Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred3=[]\n",
    "for prob in y_pred_prob_t[1]:\n",
    "    if prob > .20:\n",
    "        y_pred3.append(1)\n",
    "    else:\n",
    "        y_pred3.append(0)\n",
    "        \n",
    "print(len(y_pred3))\n",
    "print(len(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:56.809665Z",
     "start_time": "2020-10-12T18:02:56.794607Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred3.count(1)  #Actually made some predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the model metrics now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:02:58.351659Z",
     "start_time": "2020-10-12T18:02:58.329670Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test2,y_pred3)}')\n",
    "print()\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(y_test2,y_pred3)}')\n",
    "print()\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test2,y_pred3)}')\n",
    "print()\n",
    "print(f'Log Loss: {log_loss(y_test2,y_pred3)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build a model using all of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:03:00.033799Z",
     "start_time": "2020-10-12T18:02:59.977599Z"
    }
   },
   "outputs": [],
   "source": [
    "bank_all = pd.get_dummies(bank, drop_first = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:03:40.064855Z",
     "start_time": "2020-10-12T18:03:39.917969Z"
    }
   },
   "outputs": [],
   "source": [
    "# no hyper parameters for first model\n",
    "LogReg3 = LogisticRegression(penalty='l2',C=0.01)\n",
    "\n",
    "# X and y features\n",
    "X3 = bank_all.drop('y', axis =1)\n",
    "y3 = bank_all['y']\n",
    "\n",
    "# using train test split to cross val\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(X3,y3, random_state =42)\n",
    "\n",
    "# fit model\n",
    "LogReg3.fit(x_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:03:40.896561Z",
     "start_time": "2020-10-12T18:03:40.872894Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred3 = LogReg3.predict(x_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:04:03.493866Z",
     "start_time": "2020-10-12T18:04:03.475257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the metrics\n",
    "print(f'Accuracy: {accuracy_score(y_test3,y_pred3)}')\n",
    "print()\n",
    "print(f'Confusion Matrix:\\n {confusion_matrix(y_test3,y_pred3)}')\n",
    "print()\n",
    "print(f'ROC-AUC Score: {roc_auc_score(y_test3,y_pred3)}')\n",
    "print()\n",
    "print(f'Log Loss: {log_loss(y_test3,y_pred3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Use Regularization to optimize your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:28:14.261887Z",
     "start_time": "2020-10-12T18:28:12.398750Z"
    }
   },
   "outputs": [],
   "source": [
    "# X and y features\n",
    "X = bank_all.drop('y', axis =1)\n",
    "y = bank_all['y']\n",
    "\n",
    "# using train test split to cross val\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state =42)\n",
    "\n",
    "cees = [0.01, 0.1, 1.0, 10, 100]\n",
    "\n",
    "print('ROC : C')\n",
    "for c in cees:\n",
    "    logreg = LogisticRegression(penalty='l2', C=c, max_iter=2500) # set max_iter to avoid warning\n",
    "    logreg.fit(x_train,y_train)\n",
    "    y_pred = logreg.predict(x_test)\n",
    "    roc = metrics.roc_auc_score(y_test, y_pred)\n",
    "    print(roc,\" : \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T18:29:26.947236Z",
     "start_time": "2020-10-12T18:29:22.103622Z"
    }
   },
   "outputs": [],
   "source": [
    "# look ina \n",
    "cees = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7 ,1.8, 1.9]\n",
    "\n",
    "for c in cees:\n",
    "    logreg = LogisticRegression(penalty='l2', C=c, max_iter=3000)\n",
    "    logreg.fit(x_train,y_train)\n",
    "    y_pred = logreg.predict(x_test)\n",
    "    roc = roc_auc_score(y_test, y_pred)\n",
    "    print(roc,\" : \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
